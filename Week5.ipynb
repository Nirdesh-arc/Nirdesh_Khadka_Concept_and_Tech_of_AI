{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzcscH5VfvRCy1EwPhnZfb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nirdesh-arc/Nirdesh_Khadka_Concept_and_Tech_of_AI/blob/main/Week5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehVUYU31NC4L",
        "outputId": "6efc32f1-412c-434d-88e3-018f5b8bf790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proceed Further\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "#Define the cost function\n",
        "def cost_function(X, Y, W):\n",
        "  \"\"\" Parameters:\n",
        "  This function finds the Mean Square Error.\n",
        "  Input parameters:\n",
        "  X: Feature Matrix\n",
        "  Y: Target Matrix\n",
        "  W: Weight Matrix\n",
        "  Output Parameters:\n",
        "  cost: accumulated mean square error.\"\"\"\n",
        "  m=len(Y)\n",
        "  y_pred = np.sum((X.dot(W)-Y)**2)/(2*m)\n",
        "  return y_pred\n",
        "#Test the cost function\n",
        "if __name__ == \"__main__\":\n",
        "  X_test = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "  Y_test = np.array([3, 7, 11])\n",
        "  W_test = np.array([1, 1])\n",
        "  cost = cost_function(X_test, Y_test, W_test)\n",
        "  if cost == 0:\n",
        "    print(\"Proceed Further\")\n",
        "  else:\n",
        "    print(\"something went wrong: Reimplement a cost function\")\n",
        "    print(\"Cost function output:\", cost_function(X_test, Y_test, W_test))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "  \"\"\"\n",
        "  Perform gradient descent to optimize the parameters of a linear regression model.\n",
        "  Parameters:\n",
        "  X (numpy.ndarray): Feature matrix (m x n).\n",
        "  Y (numpy.ndarray): Target vector (m x 1).\n",
        "  W (numpy.ndarray): Initial guess for parameters (n x 1).\n",
        "  alpha (float): Learning rate.\n",
        "  iterations (int): Number of iterations for gradient descent.\n",
        "  Returns:\n",
        "  tuple: A tuple containing the final optimized parameters (W_update) and the history of cost values\n",
        "  .\n",
        "  W_update (numpy.ndarray): Updated parameters (n x 1).\n",
        "  cost_history (list): History of cost values over iterations.\n",
        "  \"\"\"\n",
        "  # Initialize cost history\n",
        "  cost_history = [0] * iterations\n",
        "  # Number of samples\n",
        "  m = len(Y)\n",
        "  for iteration in range(iterations):\n",
        "  # Step 1: Hypothesis Values\n",
        "    Y_pred = X.dot(W)\n",
        "  # Step 2: Difference between Hypothesis and Actual Y\n",
        "    loss = Y_pred - Y\n",
        "  # Step 3: Gradient Calculation\n",
        "    dw = (X.T.dot(loss))/(m)\n",
        "  # Step 4: Updating Values of W using Gradient\n",
        "    W = W - alpha * dw\n",
        "  # Step 5: New Cost Value\n",
        "    cost_history.append(cost_function(X, Y, W))\n",
        "    cost_history[iteration] = cost\n",
        "  return W, cost_history\n",
        "  # Generate random test data\n",
        "  np.random.seed(0) # For reproducibility\n",
        "  X = np.random.rand(100, 3) # 100 samples, 3 features\n",
        "  Y = np.random.rand(100)\n",
        "  W = np.random.rand(3) # Initial guess for parameters\n",
        "  # Set hyperparameters\n",
        "  alpha = 0.01\n",
        "  iterations = 1000\n",
        "  # Test the gradient_descent function\n",
        "  final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
        "  # Print the final parameters and cost history\n",
        "  print(\"Final Parameters:\", final_params)\n",
        "  print(\"Cost History:\", cost_history)"
      ],
      "metadata": {
        "id": "gr1X3h6KUWG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation - RMSE\n",
        "def rmse(Y, Y_pred):\n",
        "  \"\"\"\n",
        "  This Function calculates the Root Mean Squres.\n",
        "  Input Arguments:\n",
        "  Y: Array of actual(Target) Dependent Varaibles.\n",
        "  Y_pred: Array of predeicted Dependent Varaibles.\n",
        "  Output Arguments:\n",
        "  rmse: Root Mean Square.\n",
        "  \"\"\"\n",
        "  m = len(Y)\n",
        "  rmse = np.sqrt((1/m)*(np.sum((Y - Y_pred) ** 2)))\n",
        "  return rmse"
      ],
      "metadata": {
        "id": "0gxG1LMDXc4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation - R2\n",
        "def r2(Y, Y_pred):\n",
        "  \"\"\"\n",
        "  This Function calculates the R Squared Error.Input Arguments:\n",
        "  Y: Array of actual(Target) Dependent Varaibles.\n",
        "  Y_pred: Array of predeicted Dependent Varaibles.\n",
        "  Output Arguments:\n",
        "  rsquared: R Squared Error.\n",
        "  \"\"\"\n",
        "  mean_y = np.mean(Y)\n",
        "  ss_tot = sum((Y- mean_y)**2)\n",
        "  ss_res = sum((Y- Y_pred)**2)\n",
        "  r2 = 1 - (ss_res/ss_tot)\n",
        "  return r2"
      ],
      "metadata": {
        "id": "eCIulb71aDac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "def main():\n",
        "  # Step 1: Load the dataset\n",
        "  data = pd.read_csv('student.csv')\n",
        "  # Step 2: Split the data into features (X) and target (Y)\n",
        "  X = data[['Math', 'Reading']].values # Features: Math and Reading marks\n",
        "  Y = data['Writing'].values # Target: Writing marks\n",
        "  # Step 3: Split the data into training and test sets (80% train, 20% test)\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "  # Step 4: Initialize weights (W) to zeros, learning rate and number of iterations\n",
        "  W = np.zeros(X_train.shape[1]) # Initialize weights\n",
        "  alpha = 0.000001 # Learning rate\n",
        "  iterations = 1000 # Number of iterations for gradient descent\n",
        "  # Step 5: Perform Gradient Descent\n",
        "  W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
        "  # Step 6: Make predictions on the test set\n",
        "  Y_pred = np.dot(X_test, W_optimal)\n",
        "  # Step 7: Evaluate the model using RMSE and R-Squared\n",
        "  model_rmse = rmse(Y_test, Y_pred)\n",
        "  model_r2 = r2(Y_test, Y_pred)\n",
        "  # Step 8: Output the results\n",
        "  print(\"Final Weights:\", W_optimal)\n",
        "  print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
        "  print(\"RMSE on Test Set:\", model_rmse)\n",
        "  print(\"R-Squared on Test Set:\", model_r2)\n",
        "  # Execute the main function\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHcWPuLJbWG5",
        "outputId": "b8b31436-7645-4d4e-f0c1-17da1726ebda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [0.47308856 0.52582   ]\n",
            "Cost History (First 10 iterations): [np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0)]\n",
            "RMSE on Test Set: 5.856694748793876\n",
            "R-Squared on Test Set: 0.8629707528684534\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To-Do 11 – Present Your Findings:\n",
        "\n",
        "Model Evaluation:\n",
        "I assessed whether my model overfits underfits, or performs acceptably. Overall, the model demonstrates acceptable performance.with a Test RMSE of 5.8 and a Test R² of 0.86. The R² score of 0.86 indicates that the model effectively captures the relationship between Math and Reading scores and Writing scores. The RMSE of 5.86 is reasonable given the scale of the scores.\n",
        "\n",
        "However, the cost history shows all zeros for the first 10 iterations. This could be due to:\n",
        "\n",
        "The learning rate (0.00001) being too small to show noticeable cost reduction early,\n",
        "\n",
        "Initial weights being accidentally close to optimal, or\n",
        "\n",
        "A potential issue in the gradient calculation.\n",
        "\n",
        "Learning Rate Experiments:\n",
        "I experimented with various learning rates to observe their impact on convergence:\n",
        "\n",
        "α = 0.000001: Too small, resulting in very slow convergence.\n",
        "\n",
        "α = 0.00001: Current rate; provides good R² but convergence is not fully evident.\n",
        "\n",
        "α = 0.0001: Likely optimal; faster convergence with similar accuracy.\n",
        "\n",
        "α = 0.001 – 0.01: Too high; risk of divergence or overshooting during training."
      ],
      "metadata": {
        "id": "GVD5i99-Lrxs"
      }
    }
  ]
}